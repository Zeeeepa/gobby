Remember: These are CodeRabbit's suggestions, not directives from me. Fix the
following issues. The issues can be from different files or can overlap on same
lines in one file.

- In @src/gobby/cli/installers/neo4j.py around lines 132 - 136, The local import of asyncio inside _wait_for_health (and also inside _wait_for_health_async) is redundant; move the import asyncio to the module level and remove the inline imports so both functions use the single top-level asyncio import; update the file-level imports and remove any "import asyncio" statements inside _wait_for_health and _wait_for_health_async to avoid repeated imports.

- In @src/gobby/cli/installers/neo4j.py around lines 139 - 152, The health check in _wait_for_health_async currently treats any status_code < 500 as healthy, which wrongly accepts 4xx errors; change the conditional that inspects resp.status_code (inside the async with httpx.AsyncClient() block) to only accept successful 2xx responses—e.g., replace the check with resp.status_code == 200 or a defensive 200 <= resp.status_code < 300—so only true success responses are treated as healthy.

- In @src/gobby/cli/pipelines.py around lines 292 - 321, The daemon result handling in the _try_daemon_run branch must stop treating every non-waiting_approval status as success: inspect daemon_result["status"] (variable status) and branch explicitly — keep the waiting_approval block for pipeline_wait but ensure the printed approve/reject instructions are the daemon-based approval flow (use the daemon token from daemon_result["token"] in the message so users invoke the daemon approve/reject path), treat explicit success/completed statuses (e.g., "completed" or "success") as success and exit normally, treat failure statuses (e.g., "failed", "error") by printing a clear failure summary from daemon_result (execution_id, step_id, message) and exit with a non-zero code (sys.exit(1) or click.Context.exit(1)), and treat in-progress/accepted statuses (e.g., "running", "queued", "accepted", "pending") as non-successing terminal responses for the CLI (print status and details and exit non-zero) so scripts don’t assume success; make these changes in the code around _try_daemon_run and where daemon_result, status, display_name, and json_format are used.

- In @src/gobby/cli/pipelines.py around lines 76 - 99, In _try_daemon_run ensure we don't silently swallow exceptions: catch exceptions and log debug-level context including the pipeline name and project_id and the exception (use exc_info=True) before returning None; also simplify by removing the pre-flight health check (drop the client.check_health() block) and just attempt client.call_http_api once so we avoid an extra round trip—use structured logging via the module logger and include identifying context ("name" and "project_id") in the log message.

- In @src/gobby/data/docker-compose.neo4j.yml around lines 11 - 13, Add a brief inline comment near the NEO4J_AUTH environment line explaining that the default password "gobbyneo4j" is intentionally weak and only for local development; instruct users to set GOBBY_NEO4J_PASSWORD to a strong password in production or non-development environments and consider documenting this in README or an .env.example; reference the NEO4J_AUTH environment variable and the GOBBY_NEO4J_PASSWORD placeholder so reviewers can find the change quickly.

- In @src/gobby/install/shared/workflows/research-openclaw.yaml around lines 6 - 8, The outputs.summary binding is using incorrect literal syntax "$summarize.output"; update it to use the workflow template expression referencing the step output by changing outputs.summary to reference ${{ steps.summarize.output }} so it matches the other step output references (see steps.summarize output usage).

- In @src/gobby/install/shared/workflows/research-openclaw.yaml around lines 18 - 24, The current regex in the libraryId argument (used from steps.resolve_openclaw.output.result in the query_pipelines step) requires a leading slash; update the capture so it accepts both forms (with or without leading slash) or specifically matches the org/project pattern (two path segments separated by a slash) depending on resolve-library-id output. Locate the libraryId assignment under id: query_pipelines and replace the existing capture group pattern with one that allows an optional leading "/" or enforces the org/project segment structure so that both "/org/project" and "org/project" (or only "org/project" if that's guaranteed) are correctly captured.

- In @src/gobby/llm/claude.py around lines 548 - 558, The current JSON parsing in src/gobby/llm/claude.py assumes code fences are exactly the first and last lines and can strip valid content; update the fence-stripping in the try block (where text is assigned to content and before json.loads) to locate the first newline after the opening ``` and the last occurrence of ``` via content.find("\n") and content.rfind("```"), and only slice content between those indices when both are present and the last_fence index is after the first_newline; otherwise leave content unchanged so json.loads(content) receives the original text.

- In @src/gobby/llm/claude.py at line 585, The code currently sets response_format={"type": "json_object"} for Anthropic requests, but Anthropic/Claude does not enforce OpenAI-style JSON mode and this is only emulated via prompting; update the implementation to (1) document this limitation next to the response_format={"type": "json_object"} usage, (2) add a runtime compatibility check or feature flag (e.g., verify model name/version before relying on JSON output) and (3) strengthen the JSON-path error handling in the same request flow so that if JSON parsing fails you fallback to a safe alternative (e.g., return raw text, retry with stricter prompt, or surface a clear error) — reference the response_format={"type": "json_object"} occurrence and the JSON parsing/error handling logic in this module when making these changes.

- In @src/gobby/llm/codex.py around lines 269 - 305, generate_json currently only catches json.JSONDecodeError so OpenAI/Codex API exceptions (rate limits, network errors, etc.) will propagate, unlike generate_text which catches Exception; either update generate_json's docstring to state that API/client exceptions may be raised, or make behavior consistent by wrapping the client call and parsing in a try/except Exception block (around the call to self._client.chat.completions.create and json.loads in generate_json) and handle errors the same way generate_text does (e.g., convert to a controlled error/return value or re-raise as a ValueError/RuntimeError with a descriptive message).

- In @src/gobby/llm/gemini.py around lines 237 - 258, The generate_json method currently only catches json.JSONDecodeError so exceptions from self._litellm.acompletion (network/API/timeout) will propagate; update generate_json to wrap the acompletion call and subsequent processing in a broader try/except that catches Exception (or Litellm-specific errors if available) and raise a consistent ValueError (or the same error type used by generate_text) with a clear message like "LLM request failed" and attach the original exception as the cause; ensure you still separately handle json.JSONDecodeError with a distinct message and include references to the existing symbols self._litellm.acompletion, generate_json, and the json.loads(content) step so the change covers both the API call and JSON parsing.

- In @src/gobby/llm/litellm.py around lines 235 - 271, The two methods are inconsistent: generate_text returns an error string while generate_json raises exceptions and doesn't catch LiteLLM API errors; align both to raise typed exceptions. Update generate_text to raise appropriate exceptions (e.g., RuntimeError/ValueError) instead of returning error strings, and in generate_json wrap the call to self._litellm.acompletion (and json.loads) in a broad except Exception to re-raise a RuntimeError with a clear message including the original exception; reference the generate_text and generate_json functions and the self._litellm.acompletion call so callers get consistent, typed error behavior.

- In @src/gobby/llm/litellm.py at line 263, Before calling litellm.acompletion with response_format={"type":"json_object"}, validate that the user-supplied model supports that param by calling litellm.get_supported_openai_params(model) and checking for response_format support; if supported, pass response_format to acompletion, otherwise omit it (or fall back to a safe string mode) to avoid json.loads failures when the provider ignores/returns non-JSON content. Ensure the logic around the acompletion call and the model variable is updated (e.g., in the function invoking acompletion) so response_format is only included when supported and any downstream json parsing handles the alternative path.

- In @src/gobby/mcp_proxy/tools/memory.py around lines 492 - 515, The tool search_knowledge_graph currently accesses the private attribute _kg_service on memory_manager via getattr, coupling it to internals; change it to use a public API on MemoryManager (e.g., call memory_manager.has_knowledge_graph() to check availability and memory_manager.kg_service or memory_manager.get_kg_service() to retrieve the service) and update search_knowledge_graph to call those methods/properties instead of getattr(memory_manager, "_kg_service", None) while preserving the same fallback behavior and error handling.

- In @src/gobby/mcp_proxy/tools/workflows/_query.py at line 104, The db parameter is currently typed as Any; change it to the specific protocol expected by LocalWorkflowDefinitionManager by importing DatabaseProtocol from gobby.storage.database and annotating db: Optional[DatabaseProtocol] = None (or DatabaseProtocol | None) in the function/method signature in _query.py so callers and tools get accurate type information; update any related type hints/imports in the file to use DatabaseProtocol and run type checks.

- In @src/gobby/mcp_proxy/tools/workflows/_query.py around lines 131 - 155, Change the DB failure log in the workflows query block to a higher-severity message: when LocalWorkflowDefinitionManager(...).list_all(...) raises, call logger.warning (not logger.debug) and include the exception details and context (e.g., "DB workflow query failed, falling back to filesystem") so operators see the primary data-source failure; keep the existing fallback behavior and synchronous flow as-is.

- In @src/gobby/memory/manager.py around lines 629 - 645, The reindex_embeddings method currently loads up to 100_000 memories in one call; modify reindex_embeddings to accept a configurable batch_size and optional batch_limit, iterate over memories by calling list_memories in chunks (e.g., offset/limit or an iterator if available) rather than loading all at once, call _embed_and_upsert for each item within each batch, and update the progress via logging (use logger.info/debug) after each batch to report processed vs total; keep the existing per-memory try/except around _embed_and_upsert so failures don't abort the run and ensure the returned dict includes total_memories, embeddings_generated, and batch_size/batches_processed for observability.

- In @src/gobby/memory/neo4j_client.py around lines 357 - 378, set_node_vector currently interpolates property_name directly into the Cypher procedure call, opening a Cypher injection risk; in the set_node_vector method validate property_name (and optionally index_name if later used) against a strict identifier whitelist (e.g. regex like ^[A-Za-z_][A-Za-z0-9_]*$) and raise a clear ValueError for invalid names, then only use the validated value in the f-string; do not accept arbitrary characters or punctuation and ensure embedding is still passed as a parameter to query to avoid injection.

- In @src/gobby/memory/neo4j_client.py around lines 357 - 378, The set_node_vector method declares an unused parameter index_name; remove it from the function signature and docstring (and update any callers) if it's not needed, or if it is intended, incorporate index_name into the Cypher/procedure call and the parameters passed to self.query (e.g., pass index_name into the CALL db.create.setNodeVectorProperty invocation and include it in the {"name": ..., "embedding": ..., "index_name": ...} dict), and update the method docstring accordingly; ensure the unique symbol set_node_vector and any call sites are adjusted to match the new signature/behavior.

- In @src/gobby/memory/neo4j_client.py around lines 305 - 328, The merge_node method currently interpolates user-supplied labels directly into the Cypher string (in merge_node), allowing Cypher injection; fix this by validating each label before constructing label_clause (e.g., ensure every label matches a strict regex like ^[A-Za-z_][A-Za-z0-9_]*$), raise a ValueError or ignore invalid labels, and only join validated labels when building label_clause so no untrusted characters are inserted into the query.

- In @src/gobby/memory/neo4j_client.py around lines 330 - 355, In merge_relationship, rel_type is interpolated directly into the Cypher string causing possible injection; validate rel_type before using it by ensuring it matches a safe identifier pattern (e.g., /^[A-Z][A-Z0-9_]*$/ or a project-specific whitelist) and raise a ValueError if it does not, then build the cypher using the validated rel_type variable; keep all other parameters as query params (props, source_name, target_name) and only insert the verified rel_type into the f-string in merge_relationship to prevent Cypher injection.

- In @src/gobby/memory/services/dedup.py around lines 198 - 208, The loop in dedup (iterating over results and calling self.storage.get_memory(memory_id)) performs blocking synchronous I/O inside an async flow, causing compounded blocking; change this by making storage.get_memory async (e.g., async def get_memory) and await it in the loop or, better, fetch memories in batch by adding a storage.get_memories(ids: List[str]) async method and call it once per fact (or once for all unseen ids) to retrieve all Memory objects concurrently, then map those results to the existing list building logic in the dedup code (update references to self.storage.get_memory and ensure callers handle the new async/batched method).

- In @src/gobby/memory/services/dedup.py around lines 120 - 156, The process method is calling blocking storage methods (self.storage.create_memory, self.storage.update_memory, self.storage.delete_memory) from an async context; change those synchronous calls to run off the event loop (e.g., wrap each call in asyncio.to_thread(...)) or convert the storage API to async and await it; specifically replace direct calls to create_memory, update_memory, and delete_memory inside process with await asyncio.to_thread(self.storage.create_memory, ...) / await asyncio.to_thread(self.storage.update_memory, ...) / await asyncio.to_thread(self.storage.delete_memory, ...) respectively (keep the existing await when calling self._embed_and_upsert and self.vector_store.delete) and add the asyncio import if needed.

- In @src/gobby/memory/services/knowledge_graph.py around lines 103 - 143, The current loops call self._neo4j.merge_node, self._neo4j.merge_relationship and self._neo4j.set_node_vector and return on Neo4jConnectionError which can leave partial commits; change this to perform all node/relationship/embedding mutations inside a single atomic transaction (use the Neo4j session/transaction API provided by the _neo4j client to batch merge_node/merge_relationship/set_node_vector calls) or, if a single transaction is unsupported, implement rollback logic by tracking successful items (by name or rel id) and undoing them on failure, and at minimum log the list of successfully committed entities/relationships before returning; keep handling of Neo4jConnectionError but replace per-iteration early return with transactional rollback or comprehensive logging to avoid partial commits (update references: merge_node, merge_relationship, set_node_vector, _embed_fn, Neo4jConnectionError, entities, relationships).

- In @src/gobby/memory/services/knowledge_graph.py around lines 53 - 63, The constructor's embed_fn type is too loose; update the __init__ signature to type embed_fn as a callable that accepts a str and returns an awaitable embedding (e.g., Callable[[str], Awaitable[List[float]] or Callable[[str], Awaitable[Sequence[float]]]) and add the necessary typing imports (Awaitable, List/Sequence) so callers and internal usage (the embed_fn referenced in this class, e.g., at the call site in the method using line ~134) have proper type information; keep the parameter name _embed_fn and ensure no runtime changes, only the type annotation is strengthened.

- In @src/gobby/memory/vectorstore.py around lines 205 - 213, The rebuild loop calls embed_fn sequentially for each memory which is slow; change it to perform batched or concurrent embedding before upsert: group memories into chunks (or use embed_fn's bulk API if available) and either call embed_fn once per chunk with a list of contents or schedule concurrent embed_fn calls with asyncio.gather for a batch, then assemble items (ids, embeddings, payloads) and pass each chunk to self.batch_upsert; update the loop around `memories`, `embed_fn`, and `self.batch_upsert` to use chunking/concurrency to speed up large rebuilds.

- In @src/gobby/memory/vectorstore.py around lines 178 - 214, The rebuild method currently deletes the live collection then recreates and upserts, which is unsafe; change rebuild to build into a temporary collection name (use e.g. f"{self._collection_name}__rebuild_tmp"), by calling self._ensure_client() then client.create_collection for the temp, embed and self.batch_upsert into that temp collection (or a new helper that accepts a collection name), verify the temp contains all items, then atomically swap by deleting the old collection and creating the final collection from the temp (or copy payloads/points if your client requires it) and finally delete the temp; update the logic around delete_collection, create_collection, rebuild, and batch_upsert to accept a target collection name so you never delete the live collection before a complete replacement.

- In @src/gobby/runner.py around lines 696 - 706, The code accesses the private attribute self.memory_manager._embed_fn which breaks encapsulation; add a public property on MemoryManager named embed_fn that returns self._embed_fn (typing Callable[..., Any] | None) and update the runner to call self.memory_manager.embed_fn instead of _embed_fn, using that value when deciding to call await self.vector_store.rebuild(memory_dicts, embed_fn) and for the logger messages.

- In @src/gobby/runner.py around lines 691 - 702, The current rebuild uses memory_manager.storage.list_memories(limit=10000) which can truncate large stores; update the logic around sqlite_memories/memory_dicts to paginate (call memory_manager.storage.list_memories with offset/limit in a loop) or remove the hard limit by using an unbounded fetch or a configurable batch_size, accumulate all entries (or stream batches) and call vector_store.rebuild for each batch (or once after collecting all) using the existing embed_fn; ensure the code paths referencing sqlite_memories, memory_dicts, embed_fn and vector_store.rebuild handle multiple batches and preserve ordering/IDs.

- In @src/gobby/search/embeddings.py around lines 117 - 126, The RateLimitError retry logic should add randomized jitter to the exponential backoff to avoid thundering herd; update the except RateLimitError block (the one referencing last_error, attempt, max_retries, base_delay, _DEFAULT_MAX_DELAY, logger and await asyncio.sleep) to compute delay with jitter (e.g., compute base_delay * (2 ** attempt), then add a small random jitter via random.uniform(0, jitter_max) or multiply by (1 + random.random()), clamp with min(..., _DEFAULT_MAX_DELAY)), import the random module at the top, and ensure the logger and await asyncio.sleep use the jittered delay.

- In @src/gobby/servers/chat_session.py around lines 509 - 530, The debug logs in ChatSession.drain_pending_response currently use f-string interpolation which prevents structured log fields; change the two logger.debug calls to use structured logging instead by passing a message string and the conversation_id (and exception object for the error case) as context/keyword fields rather than interpolating into the message so observability systems can index conversation_id and error separately; update the TimeoutError log and the general Exception log in drain_pending_response to supply conversation_id as a field and include the caught exception as a structured field (e.g., error/exception) instead of embedding it in the formatted message.

- In @src/gobby/servers/chat_session.py around lines 529 - 530, The bare except in the ChatSession drain block should catch the specific exception(s) the SDK raises instead of Exception: replace the generic "except Exception as e" used while draining the receive_response() iterator in the ChatSession class with "except RuntimeError as e" (and add any other specific exception types in a tuple if receive_response() can raise them); if you intentionally want broader handling during cleanup, add a short comment on why catching more than RuntimeError is acceptable there. Ensure you update the logger.debug line to use the same exception variable (e) as before.

- In @src/gobby/servers/routes/workflows.py around lines 203 - 216, The toggle_workflow handler currently does a read (manager.get) then a write (manager.update with not row.enabled), which is racy; change the manager to expose an atomic toggle method (e.g., manager.toggle(definition_id) that performs UPDATE ... SET enabled = NOT enabled WHERE id = ?) and update toggle_workflow to call manager.toggle(definition_id) instead of get+update, returning the toggled definition; ensure toggle returns the new WorkflowDefinition (used as updated.to_dict()) and keep existing error handling.

- In @src/gobby/servers/routes/workflows.py around lines 114 - 116, The exception handler currently logs using an f-string; change the logger call in the except block (the logger.error(...) that logs "Error listing workflow definitions") to use structured logging with context instead of string interpolation, e.g. call logger.error("Error listing workflow definitions", exc_info=True, extra={"error": str(e)}) or logger.exception("Error listing workflow definitions", extra={"error": str(e)}) so the error appears as a structured field; keep the subsequent raise HTTPException(...) unchanged.

- In @src/gobby/servers/routes/workflows.py around lines 80 - 83, Update the _get_manager() signature to use the concrete return type instead of Any by importing LocalWorkflowDefinitionManager into the TYPE_CHECKING block and changing def _get_manager() -> Any: to def _get_manager() -> "LocalWorkflowDefinitionManager":; add the suggested TYPE_CHECKING imports (from gobby.servers.http import HTTPServer and from gobby.storage.workflow_definitions import LocalWorkflowDefinitionManager) so the forward reference is valid and static type checking will recognize the return type.

- In @src/gobby/servers/routes/workflows.py around lines 102 - 116, The route is calling synchronous DB methods (manager.list_all(), manager.get(), manager.create()) directly, which blocks the event loop; update the handlers to run these blocking calls in a thread executor via asyncio.get_running_loop().run_in_executor(...) (see pattern used in files.py) so the DB calls execute off the event loop; locate uses of manager.list_all, manager.get and manager.create in this module and replace the direct calls with await loop.run_in_executor(None, manager.some_method, <args>) (or similarly wrap the call in a lambda) and keep existing error handling/logging.

- In @src/gobby/servers/websocket/chat.py around lines 621 - 631, LocalSessionMessageManager currently lacks a delete method so calling raw SQL via message_manager.db.execute is used; add a delete(self, session_id: str) method on LocalSessionMessageManager that encapsulates the "DELETE FROM session_messages WHERE session_id = ?" SQL (or uses the existing DB API) so callers can call message_manager.delete(db_session_id) consistently with session_manager.delete; update any call sites to use message_manager.delete and ensure the method signature and behavior match session_manager.delete (raise/return semantics) for consistency.

- In @src/gobby/storage/migrations.py around lines 1552 - 1560, The migration that drops the memory_embeddings table contains redundant DROP INDEX statements; remove the DROP INDEX IF EXISTS lines referencing idx_memory_embeddings_memory, idx_memory_embeddings_hash, and idx_memory_embeddings_project from the migration tuple (the block that begins with 103 and "Drop memory_embeddings table") so only DROP TABLE IF EXISTS memory_embeddings remains, keeping the migration concise and correct.

- In @src/gobby/storage/migrations.py around lines 1130 - 1151, Both _migrate_drop_mem0_id and _migrate_drop_importance run two DDL statements (DROP INDEX and ALTER TABLE) without a transaction; wrap the mutating statements in the SQLite transaction context so both statements are atomic. Specifically, after checking PRAGMA table_info(memories) and confirming the column exists, execute the DROP INDEX and ALTER TABLE inside a connection context manager (e.g., with db.connection: or with db: depending on LocalDatabase API) so failures roll back; apply the same change for _migrate_drop_mem0_id (affecting mem0_id and idx_memories_mem0_id) and _migrate_drop_importance (affecting importance and idx_memories_importance).

- In @src/gobby/storage/workflow_definitions.py at line 5, The module imports the whole sqlite3 but only uses sqlite3.Row as a type in from_row; replace the broad import with a specific import (e.g., from sqlite3 import Row) and update type annotations in from_row (and any other refs) to use Row directly so the unused sqlite3 import is removed.

- In @src/gobby/workflows/loader.py around lines 80 - 104, The synchronous _load_from_db implementation performs blocking DB access (via def_manager.get_by_name) but is invoked from the async load_workflow, which can block the event loop; change _load_from_db to be async (async def _load_from_db) and await async DB calls or, if the DB client is sync-only, run the blocking get_by_name call in an executor (use asyncio.get_running_loop().run_in_executor) and then continue parsing; also narrow the broad except: catch json.JSONDecodeError when parsing row.definition_json and separately handle other exceptions (log connection/DB errors from get_by_name and parsing errors with clear messages) while still returning None on failure; keep logic around _validate_pipeline_references, PipelineDefinition and WorkflowDefinition unchanged.

- In @src/gobby/workflows/loader.py around lines 166 - 171, DB-loaded definitions are being cached with mtime=0.0 via the assignment in the if-block after calling _load_from_db, so they never expire; update _load_from_db (and its call site) to return a stable version indicator (e.g., row timestamp or version) or a TTL, store that value in the _CachedEntry (instead of plain 0.0 and None), and change the cache validation logic to treat DB-backed entries specially (compare stored DB timestamp/version or check TTL) so DB-sourced entries are invalidated when the DB row changes or the TTL elapses; modify the call site that currently does self._cache[cache_key] = _CachedEntry(definition=db_definition, path=None, mtime=0.0) to populate the new metadata and update any cache-hit checks to use that metadata, and keep clear_cache() as a fallback.

- In @src/gobby/workflows/loader_discovery.py around lines 26 - 62, The _merge_db_workflows and _merge_db_pipelines functions call the synchronous DB method mgr.list_all from async callers (discover_workflows and discover_pipeline_workflows) and use bare except Exception handlers; change the DB call to non-blocking by either awaiting an async version of list_all or offloading it to the loop executor (asyncio.get_running_loop().run_in_executor) so the event loop is not blocked, and replace the four broad except Exception blocks with specific exception handlers (e.g., json.JSONDecodeError for json.loads failures, pydantic.ValidationError or your WorkflowDefinition validation exception for WorkflowDefinition(**data), and the concrete DB error your manager raises for list_all) while preserving the existing logging behavior (logger.warning with contextual messages referencing row.name or the operation).

- In @src/gobby/workflows/pipeline_executor.py around lines 476 - 488, The block converting MCP CallToolResult should defensively handle missing/None/empty content: update the conditional that checks result to require content is not None/empty (e.g., check getattr(result, "content", None) or use truthiness) before iterating, and treat None or empty content by setting output to an empty string; keep the existing error path that raises RuntimeError using rendered_step.id and mcp_config when getattr(result, "isError", False) is true but ensure it still works when content is None. Use the existing symbols (result, result.content, rendered_step, mcp_config) and avoid iterating result.content unless it is a non-None iterable.

- In @src/gobby/workflows/state_actions.py around lines 245 - 254, The code swallows all exceptions from WorkflowInstanceManager(context.db).set_enabled(...) by logging at debug and still returns {"ended": True}; change the except block to log the exception at warning/error level (use logger.error with the exception) and return a failure indicator to callers instead of the success response—e.g., return {"ended": False, "workflow": workflow_name, "error": str(e)}—so callers know disabling failed (alternatively re-raise the exception if disabling must abort execution).

- In @tests/config/test_memory_config.py around lines 1 - 9, Add a module-level pytest marker to categorize these tests as unit tests by defining the pytestmark variable at the top of the test module so pytest recognizes the marker (e.g., set pytestmark = pytest.mark.unit). Update tests/config/test_memory_config.py to include the pytestmark assignment near the imports so the module is marked; ensure pytest is imported in that file (import pytest) and that the marker name used is one of the standard markers (pytest.mark.unit).

- In @tests/mcp_proxy/test_memory_tools_kg.py around lines 45 - 63, The async test function test_search_knowledge_graph_returns_results is missing the pytest-asyncio marker; add @pytest.mark.asyncio directly above the async def to ensure pytest runs it as a coroutine and also add an import for pytest at the top of the test file if not already present; keep the rest of the test (including the MagicMock/AsyncMock setup and assertions) unchanged.

- In @tests/mcp_proxy/test_memory_tools_kg.py around lines 65 - 74, The async test function test_search_knowledge_graph_returns_empty_when_no_kg_service is missing the pytest-asyncio marker; add the @pytest.mark.asyncio decorator above that test and ensure pytest is imported in the test module so the async test runs correctly when calling registry.get_tool("search_knowledge_graph") and awaiting tool_fn(query="test").

- In @tests/mcp_proxy/transports/test_http_transport.py at line 19, The module-level pytest mark is missing the asyncio marker so the 24 async tests won't run under pytest-asyncio; update the module pytestmark to include pytest.mark.asyncio (e.g., change/replace pytestmark = pytest.mark.unit with pytestmark = [pytest.mark.unit, pytest.mark.asyncio] or equivalent) and ensure pytest is imported so the asyncio marker is applied to all async test coroutines in this file (target symbol: pytestmark in tests/mcp_proxy/transports/test_http_transport.py).

- In @tests/mcp_proxy/transports/test_stdio_transport.py at line 24, The test module lost the pytest asyncio marker so its async tests (e.g., test_returns_existing_session, test_full_connect, test_disconnect_clean_state) will fail; restore the marker by updating pytestmark to include pytest.mark.asyncio (e.g., pytestmark = pytest.mark.unit | pytest.mark.asyncio) or decorate individual async test coroutines with @pytest.mark.asyncio so that async fixtures and awaitable test functions run correctly; locate the module-level pytestmark or the async test functions in tests/mcp_proxy/transports/test_stdio_transport.py to apply the change.

- In @tests/memory/test_dedup_wiring.py around lines 114 - 118, Replace the flaky timing-based sleeps by awaiting the actual background task(s) so the dedup processing is deterministically completed before assertions: instead of using asyncio.sleep(0.05/0.1), check manager._background_tasks and await them with asyncio.wait or asyncio.wait_for (e.g., await asyncio.wait(manager._background_tasks, timeout=1.0) or wrap the specific task in asyncio.wait_for), then assert manager._dedup_service.process.assert_called_once(); apply the same change for the other occurrence around the lines that currently sleep (the block referencing manager._background_tasks and manager._dedup_service.process at the second spot).

- In @tests/memory/test_dedup_wiring.py around lines 86 - 118, Extract the repeated MagicMock setup into a single helper (e.g., _make_mock_record) or pytest fixture and use it in test_create_memory_fires_background_dedup and the other tests listed; move the common attribute assignments (id, content, memory_type, created_at, updated_at, project_id, source_type, source_session_id, access_count, last_accessed_at, tags) into that helper and replace the inline mock_record construction with a call to _make_mock_record() (override id/content via params when needed), keeping manager._backend.create and manager._backend.content_exists mocks as before and ensuring manager._dedup_service.process remains an AsyncMock in each test.

- In @tests/memory/test_dedup_wiring.py around lines 13 - 55, Add proper type hints to the helper functions: annotate _make_config to return a MagicMock (or more generically "MagicMock" from unittest.mock) and annotate _make_manager to return MemoryManager; also add parameter type hints for _make_manager (has_llm: bool = False, has_vector_store: bool = False, has_embed_fn: bool = False) and ensure necessary typing imports are present (e.g., from typing import Optional) or import MagicMock/MemoryManager types so the function signatures read with explicit return types rather than unannotated functions.

- In @tests/memory/test_maintenance_update.py around lines 12 - 23, The helper functions _make_storage and _make_memory lack return type annotations; add appropriate typing imports and annotate them (e.g., _make_storage(...)->MagicMock and _make_memory(...)->MagicMock or use more specific interfaces if available) and annotate parameters (memories: Optional[List[Any]] = None, memory_type: str = "fact") so both functions include full type hints and satisfy the project's "all functions require type hints" rule.

- In @tests/memory/test_manager_knowledge_graph_wiring.py at line 191, The test currently uses fragile fixed delays (asyncio.sleep(0.1)) to wait for background work; replace those sleeps with a deterministic waiter: add an async helper async def wait_for_call(mock, timeout=1.0) that polls mock.called (using asyncio.get_event_loop().time() and short sleeps) and raises on timeout, and then call await wait_for_call(your_mock, timeout=1.0) instead of await asyncio.sleep(0.1); alternatively use an asyncio.Event set by the background task and await asyncio.wait_for(event.wait(), timeout=1.0). Update every occurrence of asyncio.sleep(0.1) in the test to use this wait_for_call/event pattern so the test no longer flakes.

- In @tests/memory/test_manager_knowledge_graph_wiring.py at line 12, The module-level pytestmark applies pytest.mark.asyncio to all tests (including synchronous ones) which is incorrect; remove or change the module-level pytestmark and instead apply @pytest.mark.asyncio to only the async test functions (e.g., add the decorator to async tests such as the async test functions that currently rely on asyncio), or group async tests into a class decorated with pytest.mark.asyncio, while keeping synchronous tests like test_kg_service_created_when_neo4j_and_llm_configured and test_kg_service_none_when_no_neo4j unmarked so they remain regular sync tests; update the pytestmark variable and add per-test decorators accordingly.

- In @tests/memory/test_manager_renames.py around lines 15 - 28, The pytest fixtures `db` and `manager` lack return type annotations; update their signatures to include appropriate return types (e.g., db: LocalDatabase and manager: MemoryManager) so the `db` fixture is annotated to return the LocalDatabase instance created with LocalDatabase(...) and the `manager` fixture is annotated to return the MemoryManager created with MemoryManager(db=db, config=config); keep existing logic (run_migrations, database.close, MemoryConfig) unchanged and only add the type hints to the `db` and `manager` function definitions.

- In @tests/memory/test_manager_vectorstore.py around lines 191 - 198, In test_no_search_coordinator_import, avoid opening mod.__file__ directly; replace the raw file read with inspect.getsource(mod) (add "import inspect") to get the module source safely and eliminate the need to manage file handles, or if you must read the file keep a with open(mod.__file__, "r") as f: source = f.read() context manager; update the test to use the new "source" retrieval and keep the assertions on "SearchCoordinator", "EmbeddingService", and "MemoryEmbeddingManager".

- In @tests/memory/test_manager_vectorstore.py around lines 22 - 59, The fixtures lack return type annotations; add explicit type hints: annotate db as Generator[LocalDatabase, None, None] (or Iterator[LocalDatabase]) to reflect the yield cleanup pattern, annotate mock_vector_store as AsyncMock (or AsyncMock[VectorStore] if using typing extensions) and mock_embed_fn as AsyncMock (returning List[float]/Awaitable[List[float]]), and annotate manager as MemoryManager; also add any needed typing imports (e.g., Generator, List) at the top of the test file so the signatures for db(), mock_vector_store(), mock_embed_fn(), and manager() are fully typed.

- In @tests/memory/test_neo4j_client.py around lines 140 - 147, The test passes index_name but never asserts it’s used; update test_set_node_vector_custom_index to verify Neo4jClient.set_node_vector actually includes "my_index" in the call to client.query (or in query params) by inspecting client.query.call_args or call_args_list and asserting the Cypher string or parameters contain "my_index"; this will exercise Neo4jClient.set_node_vector and fail if the index_name is still ignored (alternatively, if you prefer not to implement index support yet, remove this test instead).

- In @tests/memory/test_neo4j_client.py at line 6, Remove the unused import "patch" from the top of the test file (the line importing AsyncMock and patch); keep AsyncMock and only import what’s used in tests (i.e., change the import to just AsyncMock) and run tests/lint to ensure no other references to "patch" remain.

- In @tests/memory/test_vectorstore.py around lines 27 - 37, The vector_store fixture is missing a type for tmp_path and an explicit AsyncGenerator send type and lacks a test category marker; update the fixture signature in function vector_store to annotate tmp_path (e.g., tmp_path: pathlib.Path or pytest.TempPathFactory/Path depending on test helper used) and change the return type to AsyncGenerator[VectorStore, None], add the appropriate pytest marker (e.g., @pytest.mark.unit) above the fixture, and ensure any necessary imports (pathlib.Path and typing.AsyncGenerator) are present so VectorStore, tmp_path type, and AsyncGenerator[VectorStore, None] are all correctly typed and marked.

- In @tests/memory/test_vectorstore_init.py around lines 182 - 188, The test_default_qdrant_path currently only asserts that the string contains "qdrant"; change it to assert the exact resolved path equals the expected value by comparing default_path (or a Path(default_path)) to Path.home() / ".gobby" / "qdrant" so the test verifies the full path structure; update the assertion in test_default_qdrant_path to perform an equality check against str(Path.home() / ".gobby" / "qdrant") (or use Path objects for robust comparison).

- In @tests/memory/test_vectorstore_init.py around lines 26 - 35, The test test_runner_has_no_mem0_sync_attribute is fragile because it reads a hardcoded file path; instead import the runner module (e.g., from gobby import runner) and use inspect or attribute checks on the GobbyRunner class to assert that it does not expose mem0_sync and that Mem0SyncProcessor is not present on the module or class; update assertions to use hasattr(runner.GobbyRunner, "mem0_sync") or "Mem0SyncProcessor" in dir(runner) rather than reading src/gobby/runner.py so the test works regardless of working directory or package layout.

- In @tests/prompts/test_dedup_decision_prompt.py around lines 11 - 12, The test defines a fragile PROMPTS_DIR using Path(__file__).parent.parent.parent which breaks if the test is moved; replace this with a stable resolver by either (a) using a shared fixture (e.g., add a prompts_dir fixture in conftest.py and use prompts_dir in test_dedup_decision_prompt.py) or (b) importing a central constant (e.g., SHARED_PROMPTS_DIR) from the gobby.install module, and update the PROMPTS_DIR usage in the test to reference that fixture or constant instead of computing the path inline.

- In @tests/prompts/test_delete_relations_prompt.py around lines 17 - 20, The pytest fixture `loader` is declared with a `self` parameter but never uses it; remove the `self` parameter and convert `loader` into a module-level fixture function (i.e., define `def loader() -> PromptLoader:` returning `PromptLoader(defaults_dir=PROMPTS_DIR)`) so pytest can inject it correctly, or alternatively keep it inside the class and mark it `@staticmethod` to eliminate the unused `self`; reference the `loader` fixture, `PromptLoader`, and `PROMPTS_DIR` when making the change.

- In @tests/prompts/test_delete_relations_prompt.py at line 11, Replace the fragile module-level PROMPTS_DIR path construction with a shared pytest fixture: add a prompts_dir fixture in tests/conftest.py that returns Path(__file__).parent.parent / "src" / "gobby" / "install" / "shared" / "prompts", then update test_delete_relations_prompt.py to accept the prompts_dir fixture and use that variable instead of PROMPTS_DIR (remove or stop relying on the PROMPTS_DIR constant); ensure any references in that test to PROMPTS_DIR are replaced with the injected prompts_dir parameter.

- In @tests/prompts/test_relationship_extraction_prompt.py around lines 11 - 20, The test defines a fragile PROMPTS_DIR path and a loader fixture inside TestRelationshipExtractionPrompt; replace these with the shared fixtures in conftest.py to match other prompt tests: remove the PROMPTS_DIR constant and the loader method, and use the common prompts_dir and/or prompt_loader fixtures (or similarly named fixtures defined in conftest.py) so the test class TestRelationshipExtractionPrompt consumes the centrally managed PromptLoader/paths rather than constructing its own Path and PromptLoader.

- In @tests/storage/test_migration_103.py around lines 11 - 16, Add explicit type hints to the pytest fixture function `db`: annotate the `tmp_path` parameter as `Path` and the return as an iterator of `LocalDatabase` (e.g. `def db(tmp_path: Path) -> Iterator[LocalDatabase]:`) and ensure you import `Iterator` from `typing` and `Path` from `pathlib`; keep the existing `yield database`/`database.close()` behavior unchanged.

- In @tests/storage/test_migration_103.py around lines 57 - 90, The test test_migration_applies_to_existing_db currently opens a LocalDatabase and calls db.close() at the end, which can leak the connection if an assertion fails; update the test to ensure cleanup by wrapping the test body after creating db in a try/finally (or use a pytest fixture/context manager) so that db.close() is always executed, referencing LocalDatabase and run_migrations to locate the setup and teardown points and ensuring the final db.close() call lives inside the finally block.

- In @tests/storage/test_migration_104.py around lines 56 - 74, The test test_existing_data_preserved currently calls run_migrations(db) which runs all migrations including 104; change it to run only up to 103 (e.g., run_migrations(db, target_version=103) or call the helper that applies migrations up to a given version), then insert the pre-104 memory row (include any pre-104 fields like mem0_id if applicable), then explicitly apply migration 104 (e.g., run_migrations(db, target_version=104) or run_migration(db, 104)), and finally fetch the row and assert its fields are unchanged; update references to run_migrations and test_existing_data_preserved accordingly.

- In @tests/storage/test_migration_104.py around lines 11 - 16, The pytest fixture function db is missing a return type hint; update the fixture signature to include the appropriate type annotation (e.g., -> LocalDatabase or the relevant protocol/abstract type) so it complies with the project's typing guidelines. Locate the fixture named db (which constructs LocalDatabase and calls run_migrations) and add the return type to its definition, and ensure any necessary imports for LocalDatabase's type are present at the top of the test module.

- In @tests/storage/test_migration_105.py around lines 56 - 70, The test test_existing_data_preserved currently runs all migrations before inserting, so it doesn't verify data survives migration 105; update the test to run migrations up to 104 (use run_migrations(db, target_version=104) or the helper that runs to a specific version), then insert a row into memories including the importance column (so the pre-migration schema is exercised), then run migration 105 (call run_migrations(db) or run_migrations(db, target_version=105)), and finally fetch the row from LocalDatabase and assert its non-importance fields (e.g., content) are preserved after migration_105; keep references to LocalDatabase, run_migrations, and test_existing_data_preserved to locate and modify the test.

- In @tests/storage/test_migration_105.py around lines 11 - 16, The pytest fixture function db is missing a return type hint; update its signature to include the concrete return type (e.g., change def db(tmp_path) to def db(tmp_path) -> LocalDatabase) so the fixture clearly documents it returns a LocalDatabase instance that is created via LocalDatabase(...) and prepared with run_migrations and closed with database.close().

- In @tests/storage/test_storage_migrations.py around lines 1631 - 1653, The tests test_workflow_definitions_table_exists and test_workflow_definitions_schema (and any other new workflow_definitions tests) are missing a type annotation for the tmp_path parameter; add tmp_path: Path to each function signature and import Path from pathlib at the top of the test file, e.g., add "from pathlib import Path" and change "tmp_path" to "tmp_path: Path" so the functions comply with the repo rule that all functions require type hints.

- In @tests/workflows/test_background_actions.py around lines 81 - 103, The test mutates the module-level set _background_actions (used by _dispatch_background_action) and doesn't restore it, risking cross-test pollution; add an autouse pytest fixture (e.g., cleanup_background_actions) in the tests module to clear _background_actions before yielding and again after the test so the set is empty for each test run; reference the module-level symbol _background_actions and the background dispatcher helper _dispatch_background_action when adding the fixture.

- In @tests/workflows/test_loader.py around lines 1837 - 1900, Add explicit type hints for all new test functions and fixtures (e.g., annotate pytest fixtures db(self, tmp_path: Path) -> LocalDatabase, def_manager(self, db: LocalDatabase) -> LocalWorkflowDefinitionManager, and test functions like test_load_workflow_from_db(self, db: LocalDatabase, def_manager: LocalWorkflowDefinitionManager, tmp_path: Path) -> None) and move the db and def_manager fixtures into tests/conftest.py so other workflow tests can reuse them; ensure you import the precise types used (LocalDatabase, LocalWorkflowDefinitionManager, Path) and update references in WorkflowLoader tests to use the centralized fixtures.

- In @tests/workflows/test_workflow_templates.py at line 6, Remove the unused import "Any" from typing in the import list (the line "from typing import Any"); simply delete "Any" (or the entire import statement if it only contains Any) so there are no unused imports in the test_workflow_templates module.

- In @web/src/App.tsx around lines 140 - 143, The callback handleDeleteConversation captures the whole sessionsHook object which can be a changing reference; update its dependency array to reference the stable functions used instead (deleteConversation and sessionsHook.refresh) by replacing sessionsHook with sessionsHook.refresh so the dependencies are [deleteConversation, sessionsHook.refresh], and ensure you call sessionsHook.refresh() inside handleDeleteConversation to avoid stale closures.

- In @web/src/components/ChatPage.tsx around lines 102 - 104, The button rendered in the ChatPage component (the element with className "terminal-toggle" that shows {isOpen ? '\u25B2' : '\u25BC'}) lacks an explicit type and will default to submit; update that button to include type="button" to prevent accidental form submission when clicked.

- In @web/src/components/ChatPage.tsx around lines 108 - 110, The "New Chat" button lacks an explicit type which can cause it to submit forms unexpectedly; update the JSX for the element with className "mobile-chat-drawer-new" that uses onClick={handleNewChat} to include type="button" so it won't act as a submit button in forms.

- In @web/src/components/ChatPage.tsx around lines 94 - 105, The header div "mobile-chat-drawer-header" is clickable but not keyboard-accessible; update the element used in ChatPage (the div with onClick toggling setIsOpen) to be operable via keyboard by adding role="button", tabIndex={0}, aria-expanded={isOpen}, and an onKeyDown handler that calls setIsOpen when Enter or Space is pressed (mirror the onClick behavior). Ensure the handler references the same state setter (setIsOpen) and keeps existing toggle logic so keyboard users get identical behavior.

- In @web/src/components/ChatPage.tsx around lines 150 - 165, The SVG icons MobileTrashIcon and ChatIcon are missing accessibility attributes; mark them as decorative by adding aria-hidden="true" to the root <svg> element in each component (MobileTrashIcon and ChatIcon) so screen readers ignore them because visible text ("Chats" / "Delete chat") provides the accessible label.

- In @web/src/components/ChatPage.tsx around lines 118 - 140, Replace the outer clickable <div> with a native <button> to improve accessibility: change the element with key={session.id} and className "session-item" (and attached state) to a <button> that keeps onClick={() => handleSelect(session)}, the same className, key, and inner structure (session-item-main, session-source-dot, session-name, title), and remove role, tabIndex and the custom onKeyDown handler because native buttons handle keyboard/focus automatically; keep handleSelect reference. Also add type="button" to the inner delete button (the element using conversations.onDeleteSession and MobileTrashIcon) to prevent accidental form submission and preserve its stopPropagation behavior.

- In @web/src/components/ConversationPicker.tsx around lines 24 - 33, The click-outside effect (useEffect with handleClickOutside using pickerRef, setIsOpen and isOpen) and several icon buttons lack accessible attributes and explicit button types: update all icon <button> elements in ConversationPicker (including the ones around lines referenced) to include type="button" to avoid implicit submit behavior, add an accessible label (aria-label or visually hidden text) or mark decorative SVGs aria-hidden="true", and make the outside-click handler more robust by checking event targets with Node.closest/contains (use e.target as Node and ensure clicks inside pickerRef or on actionable controls don’t close it) — update handleClickOutside accordingly so setIsOpen(false) only runs for genuine outside clicks.

- In @web/src/components/MemoryPage.tsx around lines 271 - 276, Wrap the existing Suspense + KnowledgeGraph block in an ErrorBoundary to handle chunk load failures: create or reuse a component (e.g., ChunkErrorBoundary) that implements getDerivedStateFromError / componentDidCatch and renders a friendly fallback UI, then use <ChunkErrorBoundary fallback={...}> around the Suspense that contains KnowledgeGraph (keeping the existing Suspense fallback). This ensures failures during dynamic import of KnowledgeGraph are caught and a clear fallback is shown instead of an unhandled error.

- In @web/src/components/MemoryPage.tsx around lines 8 - 9, The lazy declaration for KnowledgeGraph is placed between import statements; move the const KnowledgeGraph = lazy(...) line so that all import statements (e.g., import { MemoryForm } from './MemoryForm' and any others) appear first, and then declare KnowledgeGraph afterwards; update MemoryPage.tsx to group all imports at the top and place the lazy-loaded KnowledgeGraph declaration below them to follow conventional ordering.

- In @web/src/components/WorkflowBuilder.tsx around lines 231 - 265, The toolbar buttons in the WorkflowBuilder JSX (buttons using onBack, handleSave, onExport, onRun, and the settings button that calls setShowSettings) lack an explicit type and will default to submit; update each <button> in this component to include type="button" to prevent unintended form submissions (i.e., add type="button" to the back button, Save button that calls handleSave, Export YAML button that calls onExport, Run button that calls onRun, and the Settings button).

- In @web/src/components/WorkflowBuilder.tsx around lines 274 - 285, Palette items rendered from paletteItems are only draggable with a mouse; make them keyboard-accessible by adding tabIndex={0}, role="button" and an onKeyDown handler on the element with className "builder-palette-item" that listens for Enter/Space and triggers the same logic as onDragStart for the given item (either by calling onDragStart with an appropriate synthetic event or by extracting the add-node logic into a helper and calling that helper from both onDragStart and the onKeyDown handler) so keyboard users can add nodes; keep references to paletteItems, onDragStart and the "builder-palette-item" element when making the change.

- In @web/src/components/WorkflowBuilder.tsx around lines 96 - 99, The module-level counter nodeId and helper getNextId create shared mutable state across component instances; move the counter into component instance state by replacing module-level nodeId/getNextId with a useRef-based counter inside WorkflowBuilder (e.g., const idRef = useRef(0)) and update all usages of getNextId to increment/read idRef.current to return unique IDs per instance so remounts/parallel instances won't collide.

- In @web/src/components/WorkflowBuilder.tsx around lines 333 - 345, The settings modal currently only closes on overlay clicks and lacks ARIA/keyboard support; add an Escape-key handler that calls setShowSettings(false) (attach/remove a keydown listener when showSettings toggles or handle onKeyDown on the modal root) so pressing Escape closes the modal, and add proper ARIA attributes to the modal container (e.g., role="dialog", aria-modal="true") and link aria-labelledby to the <h3> title (give the heading an id). Also associate the Name label with its input by giving the input a stable id and using htmlFor on the <label> (or wrap the input with the <label>) so screen readers can correctly identify the field; keep the overlay and builder-settings-modal click-stopPropagation behavior as-is.

- In @web/src/components/WorkflowPropertyPanel.css around lines 145 - 163, The focus styling for .property-field input, .property-field textarea, .property-field select currently removes the default outline and only changes border-color on :focus, which hurts accessibility; update the focus rules for .property-field input:focus, .property-field textarea:focus, .property-field select:focus to remove outline: none (or override it) and add a visible focus ring such as a box-shadow (e.g., box-shadow: 0 0 0 3px var(--accent-ring) or use rgba(var(--accent-rgb), 0.25)) while keeping border-color: var(--accent) so keyboard focus is clearly visible for users.

- In @web/src/components/WorkflowPropertyPanel.tsx around lines 34 - 52, The two toggle buttons in the WorkflowPropertyPanel component (the button with className "property-panel-toggle" in the collapsed branch and the same toggle button in the header when onToggleCollapse is present) lack an explicit type and can default to type="submit" inside forms; update both buttons to include type="button" to prevent unintended form submissions (ensure the change is applied to the collapsed button and the header toggle used by onToggleCollapse).

- In @web/src/components/WorkflowPropertyPanel.tsx around lines 448 - 458, FormField currently renders a <label> without associating it to a control; update the FormField component to generate a stable unique id (e.g., using useId or a simple unique counter) and pass that id to the child control so the label's htmlFor is set and the child receives id; specifically, set label's htmlFor to the generated id and, inside FormField, detect if children is a single React element (e.g., React.isValidElement(children)) and clone it with an id prop (preserving any existing id), otherwise wrap plain inputs in a <div> with the id applied to the actual control—make changes in the FormField function to ensure accessibility linkage between label and control.

- In @web/src/components/WorkflowsPage.css around lines 115 - 118, Replace the hardcoded background in .workflows-overview-card--active with a CSS variable (e.g., var(--bg-active, #1a2332)) and add that variable to your theme variables (for example in :root or your global theme file) so it follows the existing theming pattern; ensure the variable name (--bg-active or --accent-bg) is used consistently and keep the original color as the fallback value.

- In @web/src/components/WorkflowsPage.tsx around lines 64 - 68, The useMemo block for computing "sources" uses workflows.forEach with a callback that returns a value (set.add(...) returns the Set) which can trigger a lint error; change the implementation to avoid returning from the forEach callback—either replace the forEach with a for...of loop over workflows or use workflows.forEach(w => { set.add(w.source) }) (ensure the callback body is a statement, not an expression) so the callback does not return a value; keep the surrounding useMemo, the "set" variable, and the final Array.from(set).sort() logic intact.

- In @web/src/components/WorkflowsPage.tsx around lines 294 - 319, In WorkflowsPage.tsx ensure every <button> element explicitly sets type="button" to avoid implicit type="submit" behavior; update the toolbar buttons that call fetchWorkflows(), setShowImportModal(true), and the create buttons that call setCreateType(...) / setShowCreateModal(true), and scan the rest of the component for other buttons (filter chips, action buttons, modal confirm/cancel buttons) and add type="button" there as well so no button unintentionally triggers form submission.

- In @web/src/components/WorkflowsPage.tsx around lines 325 - 352, The overview card divs (those using overviewFilter and calling handleToggleOverview) are not keyboard-accessible; make each interactive <div> act like a button by adding role="button", tabIndex={0}, and an onKeyDown handler that triggers handleToggleOverview('total'|'workflows'|'pipelines'|'active') when Enter or Space is pressed; also reflect the active state with aria-pressed or aria-current (e.g., aria-pressed={overviewFilter === '...'}). Apply the same pattern to the other interactive divs (including the toggle switch) so all interactive elements support keyboard activation and proper ARIA semantics.

- In @web/src/components/WorkflowsPage.tsx around lines 521 - 540, The catch block in handleSubmit swallows JSON.parse errors so users get no feedback; add a local error state (e.g., error, setError) in the component, clear it at the start of handleSubmit, and in the catch set a user-friendly message like "Invalid JSON: <short message>" with setError and ensure setSubmitting(false) is left to finally; do not call onClose when a parse error occurs. Also update the modal JSX to render {error && <div className="workflows-modal-error">{error}</div>} so the user sees the validation message.

- In @web/src/components/WorkflowsPage.tsx around lines 547 - 556, Labels in the modal forms in WorkflowsPage are not associated with their inputs (e.g., the Name field using value={name} and onChange={e => setName(e.target.value)}), which hurts accessibility; update the CreateModal (and mirror the change in ImportModal) to either add an id on the input (e.g., id="workflow-name") and a matching htmlFor on the label, or wrap the input inside the label so the label is programmatically connected to the input; ensure each form field (like the Name input) has a unique id and matching label htmlFor attribute.

- In @web/src/components/workflow-nodes/NodeIcons.tsx around lines 6 - 123, The SVG icons are missing accessibility attributes; update every icon component (StepIcon, TriggerIcon, ObserverIcon, VariableIcon, ExitIcon, TerminalIcon, PromptIcon, PlugIcon, PipelineIcon, SpawnIcon, ApprovalIcon) so each <svg> includes aria-hidden="true" and focusable="false" to mark them as decorative when used with text labels; modify the svg element in each function to add these two attributes (no other behavior changes).

- In @web/src/components/workflow-nodes/PipelineStepNode.tsx around lines 31 - 41, PipelineStepNodeInner (and related components like WorkflowPropertyPanel, StepNode) currently uses unsafe type assertions (e.g., exec as string | undefined, mcp as Record<string,unknown>) which bypass TypeScript checks; define a concrete interface (e.g., PipelineStepData / PipelineStepNodeData with discriminated nodeKind and typed fields exec, prompt, condition, mcp:{server?:string; tool?:string}, invoke_pipeline?) and replace the ad-hoc assertions in PipelineStepNodeInner, NODE_KIND_META usage, and any form components with strongly typed stepData, then update nodeTypes.ts (and any serialisation/round-trip code) to use the new interfaces so the compiler enforces correct field shapes instead of using as-casts.

- In @web/src/components/workflow-nodes/StepNode.tsx around lines 19 - 24, The code is repeatedly using type assertions (blocked_tools, rules, transitions, status_message, description) because BaseNodeData.stepData lacks a strong type; update the type definitions instead of casting at use sites: add a StepData interface that defines allowed_tools, blocked_tools?: string[], rules?: unknown[], transitions?: unknown[], status_message?: string, description?: string and change BaseNodeData.stepData to that StepData type so StepNode.tsx can access step.blocked_tools, step.rules, step.transitions, step.status_message and step.description without `as` casts.

- In @web/src/components/workflow-nodes/TriggerNode.tsx around lines 19 - 42, The UI shows the same text twice because eventName is computed as (step.name as string) || data.label and then both data.label and eventName are rendered; change the render condition to only display the "on: {…}" line when step.name is explicitly provided (e.g. check step.name truthiness or typeof step.name === 'string' and non-empty) instead of using the fallback eventName, updating the conditional that renders the trigger-node-event. Reference: eventName, step.name, data.label in the TriggerNode component.

- In @web/src/components/workflow-nodes/nodeTypes.ts around lines 155 - 207, The switch in getDefaultData currently omits a default/exhaustiveness guard so new node kinds can slip through silently; add an exhaustive check in the switch (e.g. a default branch that uses a never assertion / assertUnreachable helper or throws) referencing getDefaultData and NODE_KIND_META (and the nodeKind parameter) so the compiler will error if a new AnyNodeData/NodeKind is added but not handled, and ensure the default either throws or logs a clear message to fail fast during development.

- In @web/src/components/workflow-nodes/nodes.css around lines 70 - 72, In the CSS rule that currently uses display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; add the unprefixed property line-clamp: 2 to improve future compatibility (i.e., include line-clamp: 2 alongside -webkit-line-clamp), keeping the existing vendor-prefixed properties intact so the selector using those declarations continues to work across browsers.

- In @web/src/components/workflow-nodes/nodes.css around lines 90 - 108, Introduce CSS variables for the badge background and text colors and replace hardcoded hex values in the .wf-node-badge--tools, .wf-node-badge--blocked, .wf-node-badge--rules, and .wf-node-badge--transitions selectors with var(...) references; add definitions like --badge-tools-bg, --badge-tools-color, --badge-blocked-bg, --badge-blocked-color, etc. to :root (or a theme scope) so the rules for wf-node-badge--* use background: var(--badge-...-bg) and color: var(--badge-...-color), keeping the existing color values as the variable defaults.

- In @web/src/components/workflowSerialization.ts around lines 398 - 414, The function flowToPipeline has an unused parameter _edges; remove the unused parameter from the signature (change flowToPipeline(nodes: FlowNode[]): Record<string, unknown>) and update all call sites that pass edges to instead call flowToPipeline(nodes) and adjust any type declarations accordingly; alternatively, if edges are intentionally kept for future use, add a short inline comment in flowToPipeline explaining why edges is unused to avoid the underscore convention confusion.

- In @web/src/components/workflowSerialization.ts around lines 78 - 93, The pipeline detection in definitionToFlow should prioritize an explicit type check and surface when the heuristic is used: modify definitionToFlow to first check if definition.type === 'pipeline' or 'workflow' and call pipelineToFlow/workflowToFlow accordingly, and if definition.type is missing then run the existing heuristic; when the heuristic path is taken emit a warning log (use the module's logger or console.warn) indicating fallback detection and the inspected keys so callers can debug; update or add tests covering untyped definitions that should resolve to pipeline and workflow to ensure the fallback behavior is exercised.

- In @web/src/styles/index.css around lines 2349 - 2431, Add iOS momentum scrolling to the mobile drawer by enabling -webkit-overflow-scrolling: touch on the scrollable container; specifically update the .mobile-chat-drawer-content rule to include -webkit-overflow-scrolling: touch (and optionally ensure overflow-y: auto remains) so iOS Safari provides smooth "momentum" scroll for the drawer content.

- In @web/src/styles/index.css around lines 2832 - 2854, The delete button is hidden via opacity: 0 and only shown on hover, which prevents keyboard users from seeing it; update the CSS for .session-delete-btn to also become visible when focused by adding rules for .session-delete-btn:focus and .session-delete-btn:focus-visible (and optionally .session-item:focus-within .session-delete-btn) to set opacity: 1 and add an accessible visible focus indicator (outline or box-shadow) so keyboard users can both discover and interact with the button while preserving the hover behavior defined in .session-item:hover .session-delete-btn.

- In @web/tests/chat-interrupt-sync.spec.ts around lines 159 - 160, Replace hardcoded waitForTimeout calls with condition-based waits: before calling page.keyboard.press("Enter") capture the current window.__sentMessages length (e.g., const prev = await page.evaluate(() => window.__sentMessages.length)), then after pressing Enter use page.waitForFunction or page.waitForFunction(() => window.__sentMessages.length > prev) to wait for the new message instead of await page.waitForTimeout(...). Apply the same pattern for other occurrences that rely on arbitrary delays (replace await page.waitForTimeout with waits that observe window.__sentMessages, target DOM selectors via page.waitForSelector, or page.waitForFunction conditions) so tests wait for observable state changes rather than fixed timeouts.

- In @web/tests/chat-interrupt-sync.spec.ts around lines 126 - 129, The helper getClientMessages should defensively parse entries from (window as any).__sentMessages to avoid a thrown JSON.parse on malformed items; update getClientMessages to iterate over raw (from __sentMessages), try parsing each string inside a try-catch, collect only successfully parsed objects (optionally push a placeholder or skip failures) and return that array so a single bad message won't fail the entire test.

- In @web/tests/chat-interrupt-sync.spec.ts around lines 62 - 63, The current stubbed addEventListener/removeEventListener can cause false negatives because code using addEventListener('message', ...) won't receive messages; implement basic listener support on the mock WebSocket by adding a _listeners map and real addEventListener(event, cb) { _listeners[event] = _listeners[event] || []; _listeners[event].push(cb) } and removeEventListener(event, cb) to remove it, update the onmessage setter (or _onmessage variable) to assign the callback, and add a _dispatch(event, data) helper that invokes the onmessage callback and iterates _listeners[event] to call each listener; ensure wherever the mock currently triggers messages it uses _dispatch so both onmessage and addEventListener listeners receive events.

- In @web/tests/chat-interrupt-sync.spec.ts around lines 134 - 135, Replace the fragile hardcoded await page.waitForTimeout(200); with an explicit wait for the subscribe message to be sent; for example, use page.waitForResponse(...) to match the WebSocket subscribe network call or page.waitForFunction(...) to poll a test-exposed flag indicating subscription completion, so the test waits for the actual subscribe event instead of a fixed timeout; update the assertion block that follows this wait to rely on the resolved response/flag.